{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maple-buice/chart-hero/blob/main/colab/transformer_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUQ1odMpqK1f"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJsmeNtbqK1g",
        "outputId": "b6129f6b-33f2-4de2-d6d3-22c4b19270cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 11 21:19:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwo6JqcWqK1g",
        "outputId": "211e7756-0498-4e27-e7e7-a972eaaa3d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/chart-hero\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up project directory\n",
        "PROJECT_DIR = '/content/drive/MyDrive/chart-hero'\n",
        "!mkdir -p {PROJECT_DIR}\n",
        "%cd {PROJECT_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0jRvg4sqK1g",
        "outputId": "25a4db15-0d4c-46d5-f7b5-e2db9af019f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chart-hero/chart-hero\n",
            "Already up to date.\n",
            "/content/drive/MyDrive/chart-hero\n",
            "/content/drive/MyDrive/chart-hero/chart-hero\n"
          ]
        }
      ],
      "source": [
        "# Clone or update repository\n",
        "import os\n",
        "if not os.path.exists('chart-hero'):\n",
        "    !git clone https://github.com/maple-buice/chart-hero.git\n",
        "else:\n",
        "    %cd chart-hero\n",
        "    !git pull\n",
        "    %cd ..\n",
        "\n",
        "%cd chart-hero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KtD-YME7qK1h"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio pytorch-lightning --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers timm wandb\n",
        "!pip install -q librosa soundfile scikit-learn pandas numpy matplotlib seaborn tqdm\n",
        "!pip install -q ipywidgets mido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibgreACNqK1h"
      },
      "source": [
        "## 2. Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Z9V6ccqK1h",
        "outputId": "23e9375e-ce3f-435b-964f-e8cafa289b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset setup...\n",
            "Google Drive already mounted at /content/drive.\n",
            "Project directory in Drive: /content/drive/MyDrive/chart-hero\n",
            "Dataset directory in Drive: /content/drive/MyDrive/chart-hero/datasets\n",
            "Dataset already successfully unzipped. Sentinel file found: /content/drive/MyDrive/chart-hero/datasets/.unzip_successful_sentinel\n",
            "Verified: Expected content 'e-gmd-v1.0.0' exists at '/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0'.\n",
            "--- Final Verification ---\n",
            "Checking existence of sentinel file: /content/drive/MyDrive/chart-hero/datasets/.unzip_successful_sentinel -> Exists\n",
            "Checking existence of ZIP file in Drive: /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0.zip -> Exists\n",
            "Checking existence of expected unzipped content: /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0 -> Exists\n",
            "Listing contents of '/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0' (first few items):\n",
            "- drummer5\n",
            "- drummer7\n",
            "- drummer6\n",
            "- e-gmd-v1.0.0.csv\n",
            "- LICENSE\n",
            "  ...\n",
            "Dataset setup cell execution complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_MOUNT_POINT = '/content/drive'\n",
        "MY_DRIVE_PATH = os.path.join(DRIVE_MOUNT_POINT, 'MyDrive')\n",
        "PROJECT_DIR_NAME = 'chart-hero' # As per your workspace structure\n",
        "PROJECT_DRIVE_PATH = os.path.join(MY_DRIVE_PATH, PROJECT_DIR_NAME)\n",
        "DATASET_DIR_NAME = 'datasets'\n",
        "DATASET_DIR = os.path.join(PROJECT_DRIVE_PATH, DATASET_DIR_NAME)\n",
        "\n",
        "# Dataset URL and paths\n",
        "# IMPORTANT: Please verify this URL and the expected unzipped content name for your specific dataset.\n",
        "DATASET_URL = \"https://storage.googleapis.com/magentadata/datasets/e-gmd/v1.0.0/e-gmd-v1.0.0.zip\"\n",
        "ZIP_FILE_NAME = os.path.basename(DATASET_URL)\n",
        "DRIVE_ZIP_PATH = os.path.join(DATASET_DIR, ZIP_FILE_NAME) # ZIP stored in Google Drive\n",
        "\n",
        "# Define the path for an expected file/folder after unzipping.\n",
        "# For e-gmd-v1.0.0.zip, it would be 'e-gmd-v1.0.0'.\n",
        "EXPECTED_UNZIPPED_CONTENT_NAME = \"e-gmd-v1.0.0\"\n",
        "EXPECTED_UNZIPPED_CONTENT_PATH = os.path.join(DATASET_DIR, EXPECTED_UNZIPPED_CONTENT_NAME)\n",
        "\n",
        "SENTINEL_FILE_NAME = \".unzip_successful_sentinel\"\n",
        "SENTINEL_FILE_PATH = os.path.join(DATASET_DIR, SENTINEL_FILE_NAME)\n",
        "\n",
        "# --- Setup ---\n",
        "print(\"Starting dataset setup...\")\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "if not os.path.exists(MY_DRIVE_PATH) or not os.path.ismount(DRIVE_MOUNT_POINT):\n",
        "    print(f\"Mounting Google Drive at {DRIVE_MOUNT_POINT}...\")\n",
        "    drive.mount(DRIVE_MOUNT_POINT, force_remount=True) # force_remount can be useful\n",
        "else:\n",
        "    print(f\"Google Drive already mounted at {DRIVE_MOUNT_POINT}.\")\n",
        "\n",
        "# 2. Create project and dataset directories in Drive if they don't exist\n",
        "os.makedirs(PROJECT_DRIVE_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "print(f\"Project directory in Drive: {PROJECT_DRIVE_PATH}\")\n",
        "print(f\"Dataset directory in Drive: {DATASET_DIR}\")\n",
        "\n",
        "\n",
        "# --- Main Logic ---\n",
        "\n",
        "# 3. Check if unzipping is already complete (sentinel file exists)\n",
        "if os.path.exists(SENTINEL_FILE_PATH):\n",
        "    print(f\"Dataset already successfully unzipped. Sentinel file found: {SENTINEL_FILE_PATH}\")\n",
        "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "        print(f\"Verified: Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Sentinel file exists, but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "        print(\"This might indicate that the unzipped content was moved or deleted after the sentinel was created.\")\n",
        "        print(\"If you encounter issues, consider deleting the sentinel file and this cell's output, then re-running.\")\n",
        "else:\n",
        "    print(f\"Sentinel file not found at {SENTINEL_FILE_PATH}. Proceeding with dataset download and/or unzip.\")\n",
        "\n",
        "    # 4. Download the dataset ZIP if it doesn't exist in Google Drive\n",
        "    if not os.path.exists(DRIVE_ZIP_PATH):\n",
        "        print(f\"ZIP file not found at {DRIVE_ZIP_PATH}. Downloading...\")\n",
        "        # Download directly to Google Drive\n",
        "        download_command = f\"wget -O '{DRIVE_ZIP_PATH}' '{DATASET_URL}'\"\n",
        "        print(f\"Executing: {download_command}\")\n",
        "        process = subprocess.run(download_command, shell=True, capture_output=True, text=True)\n",
        "        if process.returncode == 0:\n",
        "            print(f\"Download successful. ZIP file saved to {DRIVE_ZIP_PATH}\")\n",
        "        else:\n",
        "            print(\"Download failed.\")\n",
        "            print(f\"Stdout: {process.stdout}\")\n",
        "            print(f\"Stderr: {process.stderr}\")\n",
        "            # Clean up potentially incomplete ZIP file\n",
        "            if os.path.exists(DRIVE_ZIP_PATH):\n",
        "                os.remove(DRIVE_ZIP_PATH)\n",
        "                print(f\"Removed potentially incomplete ZIP file: {DRIVE_ZIP_PATH}\")\n",
        "            raise Exception(f\"Failed to download dataset from {DATASET_URL} to {DRIVE_ZIP_PATH}\")\n",
        "    else:\n",
        "        print(f\"ZIP file already exists at {DRIVE_ZIP_PATH}. Skipping download.\")\n",
        "        file_size = os.path.getsize(DRIVE_ZIP_PATH)\n",
        "        print(f\"Existing ZIP file size: {file_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "\n",
        "    # 5. Unzip the dataset from Google Drive to Google Drive\n",
        "    print(f\"Checking for expected unzipped content at: {EXPECTED_UNZIPPED_CONTENT_PATH}\")\n",
        "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' already exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "        print(\"Unzip will attempt to complete by skipping existing files (due to -nq option).\")\n",
        "    else:\n",
        "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found. Proceeding with unzip.\")\n",
        "\n",
        "    print(f\"Unzipping '{DRIVE_ZIP_PATH}' to '{DATASET_DIR}'...\")\n",
        "    # -nq: never overwrite existing files (good for resuming, quiet)\n",
        "    unzip_command = f\"unzip -nq '{DRIVE_ZIP_PATH}' -d '{DATASET_DIR}'\"\n",
        "    print(f\"Executing: {unzip_command}\")\n",
        "    process = subprocess.run(unzip_command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    # Check unzip outcome\n",
        "    # unzip return codes:\n",
        "    # 0: success\n",
        "    # 1: warning error (e.g., some files not processed, or -nq found existing files)\n",
        "    # Other codes: more serious errors\n",
        "    if process.returncode == 0 or process.returncode == 1:\n",
        "        print(\"Unzip command executed.\")\n",
        "        if process.stdout: print(f\"Unzip stdout: {process.stdout[:500]}...\") # Print some output\n",
        "        if process.stderr: print(f\"Unzip stderr: {process.stderr[:500]}...\")\n",
        "\n",
        "        # Verify expected content after unzip attempt\n",
        "        if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "            print(f\"Unzip appears successful. Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "            # Create sentinel file\n",
        "            print(f\"Creating sentinel file: {SENTINEL_FILE_PATH}\")\n",
        "            with open(SENTINEL_FILE_PATH, 'w') as f:\n",
        "                f.write(f\"Unzip completed successfully for {ZIP_FILE_NAME} into {DATASET_DIR} on {subprocess.check_output(['date']).decode('utf-8').strip()}\")\n",
        "            print(\"Sentinel file created.\")\n",
        "        else:\n",
        "            print(f\"ERROR: Unzip command finished (exit code {process.returncode}), but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' still NOT found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "            print(\"This could indicate an issue with the ZIP file structure, an empty ZIP, or the unzip process itself did not extract the main folder as expected.\")\n",
        "            print(f\"Please check the ZIP file contents and the '{EXPECTED_UNZIPPED_CONTENT_NAME}' variable if this is incorrect.\")\n",
        "            # Do NOT create sentinel file if primary content is missing.\n",
        "    else:\n",
        "        print(f\"Unzip failed with exit code {process.returncode}.\")\n",
        "        print(f\"Stdout: {process.stdout}\")\n",
        "        print(f\"Stderr: {process.stderr}\")\n",
        "        raise Exception(f\"Failed to unzip dataset from {DRIVE_ZIP_PATH}. Check logs for details.\")\n",
        "\n",
        "# --- Verification (Final Check) ---\n",
        "print(\"--- Final Verification ---\")\n",
        "print(f\"Checking existence of sentinel file: {SENTINEL_FILE_PATH} -> {'Exists' if os.path.exists(SENTINEL_FILE_PATH) else 'Not found'}\")\n",
        "print(f\"Checking existence of ZIP file in Drive: {DRIVE_ZIP_PATH} -> {'Exists' if os.path.exists(DRIVE_ZIP_PATH) else 'Not found'}\")\n",
        "print(f\"Checking existence of expected unzipped content: {EXPECTED_UNZIPPED_CONTENT_PATH} -> {'Exists' if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH) else 'Not found'}\")\n",
        "\n",
        "if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "    print(f\"Listing contents of '{EXPECTED_UNZIPPED_CONTENT_PATH}' (first few items):\")\n",
        "    try:\n",
        "        contents = os.listdir(EXPECTED_UNZIPPED_CONTENT_PATH)\n",
        "        for item in contents[:5]:\n",
        "            print(f\"- {item}\")\n",
        "        if len(contents) > 5:\n",
        "            print(\"  ...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not list contents: {e}\")\n",
        "else:\n",
        "    print(f\"Cannot list contents as '{EXPECTED_UNZIPPED_CONTENT_PATH}' does not exist.\")\n",
        "\n",
        "print(\"Dataset setup cell execution complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hBCDd7MDqK1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "ff135d27-6471-4fa3-e95e-042a3e0eef23"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mido'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1033492793>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/chart-hero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preparation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_legacy_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/chart-hero/chart-hero/model_training/data_preparation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmido\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMidiFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mido'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Prepare training data (if not already processed)\n",
        "# This cell converts the raw EGMD data to transformer-compatible format\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/chart-hero')\n",
        "\n",
        "from model_training.data_preparation import data_preparation\n",
        "from model_training.transformer_data import convert_legacy_data\n",
        "\n",
        "# Process raw EGMD data\n",
        "egmd_dir = \"/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0\"\n",
        "processed_dir = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
        "\n",
        "if not os.path.exists(processed_dir):\n",
        "    print(\"Processing raw EGMD data...\")\n",
        "\n",
        "    # Create data preparation instance\n",
        "    data_prep = data_preparation(\n",
        "        directory_path=egmd_dir,\n",
        "        dataset='egmd',\n",
        "        sample_ratio=1.0,\n",
        "        diff_threshold=1.0\n",
        "    )\n",
        "\n",
        "    # Create audio set with batching\n",
        "    data_prep.create_audio_set(\n",
        "        pad_before=0.1,\n",
        "        pad_after=0.1,\n",
        "        fix_length=10.0,  # 10 second segments\n",
        "        batching=True,\n",
        "        dir_path=processed_dir,\n",
        "        num_batches=20\n",
        "    )\n",
        "\n",
        "    print(\"Data processing completed!\")\n",
        "else:\n",
        "    print(\"Processed data already exists.\")\n",
        "\n",
        "!ls -la {processed_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4GOGPreqK1h"
      },
      "source": [
        "## 3. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tSmGXtnAqK1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "29cf7a57-a517-44f5-c324-243a39e06c05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaple-buice\u001b[0m (\u001b[33mmbuice-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Set up W&B logging\n",
        "import wandb\n",
        "\n",
        "# Login to W&B (you'll need to provide your API key)\n",
        "wandb.login()\n",
        "\n",
        "# Or set the API key directly\n",
        "# wandb.login(key=\"your-wandb-api-key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1fe08HFvqK1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bd7288-9802-4e82-a45e-5ab04b799c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:==================================================\n",
            "INFO:__main__:TRANSFORMER SETUP TESTS\n",
            "INFO:__main__:==================================================\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Configuration test...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing configuration classes...\n",
            "ERROR:__main__:✗ Config test failed: MPS not available but config specifies MPS device\n",
            "ERROR:__main__:✗ Configuration test FAILED\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Model Architecture test...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing model architecture...\n",
            "Model created with 7,210,761 total parameters\n",
            "Trainable parameters: 7,210,761\n",
            "INFO:__main__:✓ Input shape: torch.Size([2, 1, 256, 128])\n",
            "INFO:__main__:✓ Output logits shape: torch.Size([2, 9])\n",
            "INFO:__main__:✓ CLS embedding shape: torch.Size([2, 384])\n",
            "INFO:__main__:✓ Model Architecture test PASSED\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Data Processing test...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing data processing...\n",
            "ERROR:__main__:✗ Data processing test failed: Detected that PyTorch and TorchAudio were compiled with different CUDA versions. PyTorch has CUDA version 11.8 whereas TorchAudio has CUDA version 12.4. Please install the TorchAudio version that matches your PyTorch version.\n",
            "ERROR:__main__:✗ Data Processing test FAILED\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Training Module test...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing training module...\n",
            "ERROR:__main__:✗ Training module test failed: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.\n",
            "ERROR:__main__:✗ Training Module test FAILED\n",
            "INFO:__main__:\n",
            "==================================================\n",
            "INFO:__main__:TEST SUMMARY\n",
            "INFO:__main__:==================================================\n",
            "INFO:__main__:Configuration: FAIL\n",
            "INFO:__main__:Model Architecture: PASS\n",
            "INFO:__main__:Data Processing: FAIL\n",
            "INFO:__main__:Training Module: FAIL\n",
            "INFO:__main__:\n",
            "Overall: 1/4 tests passed\n",
            "ERROR:__main__:❌ Some tests failed. Please check the errors above.\n"
          ]
        }
      ],
      "source": [
        "# Test transformer setup\n",
        "!python model_training/test_transformer_setup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SFZlBqCqK1h"
      },
      "outputs": [],
      "source": [
        "# Start training with cloud configuration\n",
        "DATA_DIR = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0\"\n",
        "\n",
        "!python model_training/train_transformer.py \\\n",
        "    --config cloud \\\n",
        "    --data-dir {DATA_DIR} \\\n",
        "    --audio-dir {AUDIO_DIR} \\\n",
        "    --project-name chart-hero-transformer-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bDkyd8nqK1h"
      },
      "source": [
        "## 4. Resume Training (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TD03wwEqK1h"
      },
      "outputs": [],
      "source": [
        "# Resume from checkpoint\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/chart-hero/models/last.ckpt\"\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    !python model_training/train_transformer.py \\\n",
        "        --config cloud \\\n",
        "        --data-dir {DATA_DIR} \\\n",
        "        --audio-dir {AUDIO_DIR} \\\n",
        "        --resume {CHECKPOINT_PATH} \\\n",
        "        --project-name chart-hero-transformer-colab\n",
        "else:\n",
        "    print(f\"Checkpoint not found: {CHECKPOINT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_jtaL-mqK1h"
      },
      "source": [
        "## 5. Model Evaluation and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AaRwU8F9qK1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2aa6e56-7da3-4d51-907e-db54a8fb21a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model not found: /content/drive/MyDrive/chart-hero/models/best_model.ckpt\n"
          ]
        }
      ],
      "source": [
        "# Load and evaluate best model\n",
        "import torch\n",
        "from model_training.train_transformer import DrumTranscriptionModule\n",
        "from model_training.transformer_config import get_config\n",
        "\n",
        "config = get_config(\"cloud\")\n",
        "best_model_path = \"/content/drive/MyDrive/chart-hero/models/best_model.ckpt\"\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    model = DrumTranscriptionModule.load_from_checkpoint(best_model_path)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Export to ONNX for deployment\n",
        "    dummy_input = torch.randn(1, 1, 256, 128)\n",
        "    onnx_path = \"/content/drive/MyDrive/chart-hero/models/drum_transformer.onnx\"\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.model,\n",
        "        dummy_input,\n",
        "        onnx_path,\n",
        "        export_params=True,\n",
        "        opset_version=11,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['spectrogram'],\n",
        "        output_names=['logits'],\n",
        "        dynamic_axes={\n",
        "            'spectrogram': {0: 'batch_size', 2: 'time'},\n",
        "            'logits': {0: 'batch_size'}\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"Model exported to ONNX: {onnx_path}\")\n",
        "else:\n",
        "    print(f\"Best model not found: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9bLGHPNqK1h"
      },
      "source": [
        "## 6. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSpRRAa1qK1h"
      },
      "outputs": [],
      "source": [
        "# Clean up temporary files and finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "# Show final model and log locations\n",
        "print(\"Training completed!\")\n",
        "print(f\"Models saved to: /content/drive/MyDrive/chart-hero/models/\")\n",
        "print(f\"Logs saved to: /content/drive/MyDrive/chart-hero/logs/\")\n",
        "print(f\"Datasets saved to: /content/drive/MyDrive/chart-hero/datasets/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}