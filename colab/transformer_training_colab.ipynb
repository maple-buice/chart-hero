{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maple-buice/chart-hero/blob/main/colab/transformer_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUQ1odMpqK1f"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJsmeNtbqK1g",
        "outputId": "b6129f6b-33f2-4de2-d6d3-22c4b19270cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 11 21:19:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwo6JqcWqK1g",
        "outputId": "211e7756-0498-4e27-e7e7-a972eaaa3d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/chart-hero\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up project directory\n",
        "PROJECT_DIR = '/content/drive/MyDrive/chart-hero'\n",
        "!mkdir -p {PROJECT_DIR}\n",
        "%cd {PROJECT_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0jRvg4sqK1g",
        "outputId": "6eea2514-f20b-4043-d3c0-fa1534b879d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chart-hero'...\n",
            "remote: Enumerating objects: 390, done.\u001b[K\n",
            "remote: Counting objects: 100% (390/390), done.\u001b[K\n",
            "remote: Compressing objects: 100% (266/266), done.\u001b[K\n",
            "remote: Total 390 (delta 204), reused 276 (delta 108), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (390/390), 4.33 MiB | 10.56 MiB/s, done.\n",
            "Resolving deltas: 100% (204/204), done.\n",
            "/content/drive/MyDrive/chart-hero/chart-hero/chart-hero/chart-hero\n"
          ]
        }
      ],
      "source": [
        "# Clone or update repository\n",
        "import os\n",
        "if not os.path.exists('chart-hero'):\n",
        "    !git clone https://github.com/maple-buice/chart-hero.git\n",
        "else:\n",
        "    %cd chart-hero\n",
        "    !git pull\n",
        "    %cd ..\n",
        "\n",
        "%cd chart-hero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KtD-YME7qK1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb4608d-53bf-4a0e-9dcd-47e8f577b0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.7.1+cu118\n",
            "Uninstalling torch-2.7.1+cu118:\n",
            "  Successfully uninstalled torch-2.7.1+cu118\n",
            "Found existing installation: torchvision 0.22.1+cu118\n",
            "Uninstalling torchvision-0.22.1+cu118:\n",
            "  Successfully uninstalled torchvision-0.22.1+cu118\n",
            "Found existing installation: torchaudio 2.7.1+cu118\n",
            "Uninstalling torchaudio-2.7.1+cu118:\n",
            "  Successfully uninstalled torchaudio-2.7.1+cu118\n",
            "Found existing installation: fastai 2.7.19\n",
            "Uninstalling fastai-2.7.19:\n",
            "  Successfully uninstalled fastai-2.7.19\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip uninstall -y torch torchvision torchaudio fastai\n",
        "!pip install -q torch torchvision torchaudio pytorch-lightning --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers timm wandb\n",
        "!pip install -q librosa soundfile scikit-learn pandas numpy matplotlib seaborn tqdm\n",
        "!pip install -q ipywidgets mido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibgreACNqK1h"
      },
      "source": [
        "## 2. Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Z9V6ccqK1h",
        "outputId": "b02ea654-5cd4-4b8e-ea8a-63e0c07acedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset setup...\n",
            "Google Drive already mounted at /content/drive.\n",
            "Project directory in Drive: /content/drive/MyDrive/chart-hero\n",
            "Dataset directory in Drive: /content/drive/MyDrive/chart-hero/datasets\n",
            "Dataset already successfully unzipped. Sentinel file found: /content/drive/MyDrive/chart-hero/datasets/.unzip_successful_sentinel\n",
            "Verified: Expected content 'e-gmd-v1.0.0' exists at '/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0'.\n",
            "--- Final Verification ---\n",
            "Checking existence of sentinel file: /content/drive/MyDrive/chart-hero/datasets/.unzip_successful_sentinel -> Exists\n",
            "Checking existence of ZIP file in Drive: /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0.zip -> Exists\n",
            "Checking existence of expected unzipped content: /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0 -> Exists\n",
            "Listing contents of '/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0' (first few items):\n",
            "- drummer5\n",
            "- drummer7\n",
            "- drummer6\n",
            "- e-gmd-v1.0.0.csv\n",
            "- LICENSE\n",
            "  ...\n",
            "Dataset setup cell execution complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_MOUNT_POINT = '/content/drive'\n",
        "MY_DRIVE_PATH = os.path.join(DRIVE_MOUNT_POINT, 'MyDrive')\n",
        "PROJECT_DIR_NAME = 'chart-hero' # As per your workspace structure\n",
        "PROJECT_DRIVE_PATH = os.path.join(MY_DRIVE_PATH, PROJECT_DIR_NAME)\n",
        "DATASET_DIR_NAME = 'datasets'\n",
        "DATASET_DIR = os.path.join(PROJECT_DRIVE_PATH, DATASET_DIR_NAME)\n",
        "\n",
        "# Dataset URL and paths\n",
        "# IMPORTANT: Please verify this URL and the expected unzipped content name for your specific dataset.\n",
        "DATASET_URL = \"https://storage.googleapis.com/magentadata/datasets/e-gmd/v1.0.0/e-gmd-v1.0.0.zip\"\n",
        "ZIP_FILE_NAME = os.path.basename(DATASET_URL)\n",
        "DRIVE_ZIP_PATH = os.path.join(DATASET_DIR, ZIP_FILE_NAME) # ZIP stored in Google Drive\n",
        "\n",
        "# Define the path for an expected file/folder after unzipping.\n",
        "# For e-gmd-v1.0.0.zip, it would be 'e-gmd-v1.0.0'.\n",
        "EXPECTED_UNZIPPED_CONTENT_NAME = \"e-gmd-v1.0.0\"\n",
        "EXPECTED_UNZIPPED_CONTENT_PATH = os.path.join(DATASET_DIR, EXPECTED_UNZIPPED_CONTENT_NAME)\n",
        "\n",
        "SENTINEL_FILE_NAME = \".unzip_successful_sentinel\"\n",
        "SENTINEL_FILE_PATH = os.path.join(DATASET_DIR, SENTINEL_FILE_NAME)\n",
        "\n",
        "# --- Setup ---\n",
        "print(\"Starting dataset setup...\")\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "if not os.path.exists(MY_DRIVE_PATH) or not os.path.ismount(DRIVE_MOUNT_POINT):\n",
        "    print(f\"Mounting Google Drive at {DRIVE_MOUNT_POINT}...\")\n",
        "    drive.mount(DRIVE_MOUNT_POINT, force_remount=True) # force_remount can be useful\n",
        "else:\n",
        "    print(f\"Google Drive already mounted at {DRIVE_MOUNT_POINT}.\")\n",
        "\n",
        "# 2. Create project and dataset directories in Drive if they don't exist\n",
        "os.makedirs(PROJECT_DRIVE_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "print(f\"Project directory in Drive: {PROJECT_DRIVE_PATH}\")\n",
        "print(f\"Dataset directory in Drive: {DATASET_DIR}\")\n",
        "\n",
        "\n",
        "# --- Main Logic ---\n",
        "\n",
        "# 3. Check if unzipping is already complete (sentinel file exists)\n",
        "if os.path.exists(SENTINEL_FILE_PATH):\n",
        "    print(f\"Dataset already successfully unzipped. Sentinel file found: {SENTINEL_FILE_PATH}\")\n",
        "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "        print(f\"Verified: Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Sentinel file exists, but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "        print(\"This might indicate that the unzipped content was moved or deleted after the sentinel was created.\")\n",
        "        print(\"If you encounter issues, consider deleting the sentinel file and this cell's output, then re-running.\")\n",
        "else:\n",
        "    print(f\"Sentinel file not found at {SENTINEL_FILE_PATH}. Proceeding with dataset download and/or unzip.\")\n",
        "\n",
        "    # 4. Download the dataset ZIP if it doesn't exist in Google Drive\n",
        "    if not os.path.exists(DRIVE_ZIP_PATH):\n",
        "        print(f\"ZIP file not found at {DRIVE_ZIP_PATH}. Downloading...\")\n",
        "        # Download directly to Google Drive\n",
        "        download_command = f\"wget -O '{DRIVE_ZIP_PATH}' '{DATASET_URL}'\"\n",
        "        print(f\"Executing: {download_command}\")\n",
        "        process = subprocess.run(download_command, shell=True, capture_output=True, text=True)\n",
        "        if process.returncode == 0:\n",
        "            print(f\"Download successful. ZIP file saved to {DRIVE_ZIP_PATH}\")\n",
        "        else:\n",
        "            print(\"Download failed.\")\n",
        "            print(f\"Stdout: {process.stdout}\")\n",
        "            print(f\"Stderr: {process.stderr}\")\n",
        "            # Clean up potentially incomplete ZIP file\n",
        "            if os.path.exists(DRIVE_ZIP_PATH):\n",
        "                os.remove(DRIVE_ZIP_PATH)\n",
        "                print(f\"Removed potentially incomplete ZIP file: {DRIVE_ZIP_PATH}\")\n",
        "            raise Exception(f\"Failed to download dataset from {DATASET_URL} to {DRIVE_ZIP_PATH}\")\n",
        "    else:\n",
        "        print(f\"ZIP file already exists at {DRIVE_ZIP_PATH}. Skipping download.\")\n",
        "        file_size = os.path.getsize(DRIVE_ZIP_PATH)\n",
        "        print(f\"Existing ZIP file size: {file_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "\n",
        "    # 5. Unzip the dataset from Google Drive to Google Drive\n",
        "    print(f\"Checking for expected unzipped content at: {EXPECTED_UNZIPPED_CONTENT_PATH}\")\n",
        "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' already exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "        print(\"Unzip will attempt to complete by skipping existing files (due to -nq option).\")\n",
        "    else:\n",
        "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found. Proceeding with unzip.\")\n",
        "\n",
        "    print(f\"Unzipping '{DRIVE_ZIP_PATH}' to '{DATASET_DIR}'...\")\n",
        "    # -nq: never overwrite existing files (good for resuming, quiet)\n",
        "    unzip_command = f\"unzip -nq '{DRIVE_ZIP_PATH}' -d '{DATASET_DIR}'\"\n",
        "    print(f\"Executing: {unzip_command}\")\n",
        "    process = subprocess.run(unzip_command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    # Check unzip outcome\n",
        "    # unzip return codes:\n",
        "    # 0: success\n",
        "    # 1: warning error (e.g., some files not processed, or -nq found existing files)\n",
        "    # Other codes: more serious errors\n",
        "    if process.returncode == 0 or process.returncode == 1:\n",
        "        print(\"Unzip command executed.\")\n",
        "        if process.stdout: print(f\"Unzip stdout: {process.stdout[:500]}...\") # Print some output\n",
        "        if process.stderr: print(f\"Unzip stderr: {process.stderr[:500]}...\")\n",
        "\n",
        "        # Verify expected content after unzip attempt\n",
        "        if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "            print(f\"Unzip appears successful. Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "            # Create sentinel file\n",
        "            print(f\"Creating sentinel file: {SENTINEL_FILE_PATH}\")\n",
        "            with open(SENTINEL_FILE_PATH, 'w') as f:\n",
        "                f.write(f\"Unzip completed successfully for {ZIP_FILE_NAME} into {DATASET_DIR} on {subprocess.check_output(['date']).decode('utf-8').strip()}\")\n",
        "            print(\"Sentinel file created.\")\n",
        "        else:\n",
        "            print(f\"ERROR: Unzip command finished (exit code {process.returncode}), but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' still NOT found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "            print(\"This could indicate an issue with the ZIP file structure, an empty ZIP, or the unzip process itself did not extract the main folder as expected.\")\n",
        "            print(f\"Please check the ZIP file contents and the '{EXPECTED_UNZIPPED_CONTENT_NAME}' variable if this is incorrect.\")\n",
        "            # Do NOT create sentinel file if primary content is missing.\n",
        "    else:\n",
        "        print(f\"Unzip failed with exit code {process.returncode}.\")\n",
        "        print(f\"Stdout: {process.stdout}\")\n",
        "        print(f\"Stderr: {process.stderr}\")\n",
        "        raise Exception(f\"Failed to unzip dataset from {DRIVE_ZIP_PATH}. Check logs for details.\")\n",
        "\n",
        "# --- Verification (Final Check) ---\n",
        "print(\"--- Final Verification ---\")\n",
        "print(f\"Checking existence of sentinel file: {SENTINEL_FILE_PATH} -> {'Exists' if os.path.exists(SENTINEL_FILE_PATH) else 'Not found'}\")\n",
        "print(f\"Checking existence of ZIP file in Drive: {DRIVE_ZIP_PATH} -> {'Exists' if os.path.exists(DRIVE_ZIP_PATH) else 'Not found'}\")\n",
        "print(f\"Checking existence of expected unzipped content: {EXPECTED_UNZIPPED_CONTENT_PATH} -> {'Exists' if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH) else 'Not found'}\")\n",
        "\n",
        "if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "    print(f\"Listing contents of '{EXPECTED_UNZIPPED_CONTENT_PATH}' (first few items):\")\n",
        "    try:\n",
        "        contents = os.listdir(EXPECTED_UNZIPPED_CONTENT_PATH)\n",
        "        for item in contents[:5]:\n",
        "            print(f\"- {item}\")\n",
        "        if len(contents) > 5:\n",
        "            print(\"  ...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not list contents: {e}\")\n",
        "else:\n",
        "    print(f\"Cannot list contents as '{EXPECTED_UNZIPPED_CONTENT_PATH}' does not exist.\")\n",
        "\n",
        "print(\"Dataset setup cell execution complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBCDd7MDqK1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "ff135d27-6471-4fa3-e95e-042a3e0eef23"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mido'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1033492793>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/chart-hero'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preparation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_legacy_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/chart-hero/chart-hero/model_training/data_preparation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmido\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMidiFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mido'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Prepare training data (if not already processed)\n",
        "# This cell converts the raw EGMD data to transformer-compatible format\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/chart-hero')\n",
        "\n",
        "from model_training.data_preparation import data_preparation\n",
        "from model_training.transformer_data import convert_legacy_data\n",
        "\n",
        "# Process raw EGMD data\n",
        "egmd_dir = \"/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0\"\n",
        "processed_dir = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
        "\n",
        "if not os.path.exists(processed_dir):\n",
        "    print(\"Processing raw EGMD data...\")\n",
        "\n",
        "    # Create data preparation instance\n",
        "    data_prep = data_preparation(\n",
        "        directory_path=egmd_dir,\n",
        "        dataset='egmd',\n",
        "        sample_ratio=1.0,\n",
        "        diff_threshold=1.0\n",
        "    )\n",
        "\n",
        "    # Create audio set with batching\n",
        "    data_prep.create_audio_set(\n",
        "        pad_before=0.1,\n",
        "        pad_after=0.1,\n",
        "        fix_length=10.0,  # 10 second segments\n",
        "        batching=True,\n",
        "        dir_path=processed_dir,\n",
        "        num_batches=20\n",
        "    )\n",
        "\n",
        "    print(\"Data processing completed!\")\n",
        "else:\n",
        "    print(\"Processed data already exists.\")\n",
        "\n",
        "!ls -la {processed_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4GOGPreqK1h"
      },
      "source": [
        "## 3. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tSmGXtnAqK1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22257298-4806-413d-e507-ccfcac52f488"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Set up W&B logging\n",
        "import wandb\n",
        "\n",
        "# Login to W&B (you'll need to provide your API key)\n",
        "wandb.login()\n",
        "\n",
        "# Or set the API key directly\n",
        "# wandb.login(key=\"your-wandb-api-key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1fe08HFvqK1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50028d1-fee6-4744-965e-1355a35bf843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:==================================================\n",
            "INFO:__main__:TRANSFORMER SETUP TESTS\n",
            "INFO:__main__:==================================================\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Configuration test (using 'cloud' config context)...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing configuration classes...\n",
            "INFO:__main__:--- Validating invoked config: 'cloud' ---\n",
            "INFO:__main__:✓ Invoked config 'cloud' (resolved to cloud): VALIDATED - device cuda, batch_size=64\n",
            "INFO:__main__:--- Checking standard config: 'local' ---\n",
            "WARNING:__main__:✓ Config 'local' (device: mps) specifies MPS, but MPS not available. Loaded but skipping full device validation for this non-invoked config.\n",
            "INFO:__main__:--- Checking standard config: 'auto' ---\n",
            "INFO:__main__:Auto-detected config for verification: CloudConfig\n",
            "INFO:__main__:✓ Config 'auto': Loaded & Validated - device cuda, batch_size=64\n",
            "INFO:__main__:✓ Configuration test PASSED\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Model Architecture test (using 'cloud' config context)...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing model architecture...\n",
            "Model created with 7,214,217 total parameters\n",
            "Trainable parameters: 7,214,217\n",
            "INFO:__main__:✓ Input shape: torch.Size([2, 1, 256, 128])\n",
            "INFO:__main__:✓ Output logits shape: torch.Size([2, 9])\n",
            "INFO:__main__:✓ CLS embedding shape: torch.Size([2, 384])\n",
            "INFO:__main__:✓ Model Architecture test PASSED\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Data Processing test (using 'cloud' config context)...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing data processing...\n",
            "INFO:__main__:✓ Audio shape: torch.Size([1, 264600])\n",
            "INFO:__main__:✓ Spectrogram shape: torch.Size([1, 517, 128])\n",
            "INFO:__main__:✓ Padded spectrogram shape: torch.Size([1, 528, 128])\n",
            "INFO:__main__:✓ Patch shape: (33, 8)\n",
            "INFO:model_training.transformer_data:Created test dataset with 5 samples\n",
            "INFO:__main__:✓ Dataset created with 5 samples\n",
            "WARNING:model_training.transformer_data:Error processing sample 0: 'audio_wav'\n",
            "INFO:__main__:✓ Sample spectrogram shape: torch.Size([1, 528, 128])\n",
            "INFO:__main__:✓ Sample labels shape: torch.Size([9])\n",
            "INFO:__main__:✓ Data Processing test PASSED\n",
            "INFO:__main__:\n",
            "------------------------------\n",
            "INFO:__main__:Running Training Module test (using 'cloud' config context)...\n",
            "INFO:__main__:------------------------------\n",
            "INFO:__main__:Testing training module...\n",
            "Model created with 7,214,217 total parameters\n",
            "Trainable parameters: 7,214,217\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py:447: You are trying to `self.log()` but `Trainer(barebones=True)` is configured. Logging can impact raw speed so it is disabled under this setting.\n",
            "INFO:__main__:✓ Training step loss: 1.0295\n",
            "INFO:__main__:✓ Validation step loss: 1.0637\n",
            "INFO:__main__:✓ Optimizer configured: AdamW, Scheduler: SequentialLR\n",
            "INFO:__main__:✓ Training Module test PASSED\n",
            "INFO:__main__:\n",
            "==================================================\n",
            "INFO:__main__:TEST SUMMARY\n",
            "INFO:__main__:==================================================\n",
            "INFO:__main__:Configuration: PASS\n",
            "INFO:__main__:Model Architecture: PASS\n",
            "INFO:__main__:Data Processing: PASS\n",
            "INFO:__main__:Training Module: PASS\n",
            "INFO:__main__:\n",
            "Overall: 4/4 tests passed\n",
            "INFO:__main__:🎉 All tests passed! Transformer setup is ready.\n"
          ]
        }
      ],
      "source": [
        "# Test transformer setup\n",
        "!python model_training/test_transformer_setup.py --config cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1SFZlBqCqK1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e35418-a60b-44aa-ed68-1007a4db5e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train_transformer.py [-h] [--config CONFIG] [--use-wandb]\n",
            "                            [--quick-test] [--debug]\n",
            "                            [--experiment-tag EXPERIMENT_TAG]\n",
            "                            [--data-dir DATA_DIR] [--audio-dir AUDIO_DIR]\n",
            "                            [--monitor-gpu] [--batch-size BATCH_SIZE]\n",
            "                            [--hidden-size HIDDEN_SIZE]\n",
            "                            [--learning-rate LEARNING_RATE]\n",
            "                            [--num-workers NUM_WORKERS]\n",
            "                            [--accumulate-grad-batches ACCUMULATE_GRAD_BATCHES]\n",
            "train_transformer.py: error: unrecognized arguments: --project-name chart-hero-transformer-colab\n"
          ]
        }
      ],
      "source": [
        "# Start training with cloud configuration\n",
        "DATA_DIR = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0\"\n",
        "\n",
        "!python model_training/train_transformer.py \\\n",
        "    --config cloud \\\n",
        "    --data-dir {DATA_DIR} \\\n",
        "    --audio-dir {AUDIO_DIR} \\\n",
        "    --project-name chart-hero-transformer-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bDkyd8nqK1h"
      },
      "source": [
        "## 4. Resume Training (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TD03wwEqK1h"
      },
      "outputs": [],
      "source": [
        "# Resume from checkpoint\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/chart-hero/models/last.ckpt\"\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    !python model_training/train_transformer.py \\\n",
        "        --config cloud \\\n",
        "        --data-dir {DATA_DIR} \\\n",
        "        --audio-dir {AUDIO_DIR} \\\n",
        "        --resume {CHECKPOINT_PATH} \\\n",
        "        --project-name chart-hero-transformer-colab\n",
        "else:\n",
        "    print(f\"Checkpoint not found: {CHECKPOINT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_jtaL-mqK1h"
      },
      "source": [
        "## 5. Model Evaluation and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaRwU8F9qK1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2aa6e56-7da3-4d51-907e-db54a8fb21a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model not found: /content/drive/MyDrive/chart-hero/models/best_model.ckpt\n"
          ]
        }
      ],
      "source": [
        "# Load and evaluate best model\n",
        "import torch\n",
        "from model_training.train_transformer import DrumTranscriptionModule\n",
        "from model_training.transformer_config import get_config\n",
        "\n",
        "config = get_config(\"cloud\")\n",
        "best_model_path = \"/content/drive/MyDrive/chart-hero/models/best_model.ckpt\"\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    model = DrumTranscriptionModule.load_from_checkpoint(best_model_path)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Export to ONNX for deployment\n",
        "    dummy_input = torch.randn(1, 1, 256, 128)\n",
        "    onnx_path = \"/content/drive/MyDrive/chart-hero/models/drum_transformer.onnx\"\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.model,\n",
        "        dummy_input,\n",
        "        onnx_path,\n",
        "        export_params=True,\n",
        "        opset_version=11,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['spectrogram'],\n",
        "        output_names=['logits'],\n",
        "        dynamic_axes={\n",
        "            'spectrogram': {0: 'batch_size', 2: 'time'},\n",
        "            'logits': {0: 'batch_size'}\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"Model exported to ONNX: {onnx_path}\")\n",
        "else:\n",
        "    print(f\"Best model not found: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9bLGHPNqK1h"
      },
      "source": [
        "## 6. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSpRRAa1qK1h"
      },
      "outputs": [],
      "source": [
        "# Clean up temporary files and finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "# Show final model and log locations\n",
        "print(\"Training completed!\")\n",
        "print(f\"Models saved to: /content/drive/MyDrive/chart-hero/models/\")\n",
        "print(f\"Logs saved to: /content/drive/MyDrive/chart-hero/logs/\")\n",
        "print(f\"Datasets saved to: /content/drive/MyDrive/chart-hero/datasets/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}