{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/maple-buice/chart-hero/blob/main/colab/transformer_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUQ1odMpqK1f"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJsmeNtbqK1g",
    "outputId": "455e586a-8b5a-4ff2-c5dc-b772eb2aedd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n",
      "CUDA available: False\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\n")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\n")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\n")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kwo6JqcWqK1g",
    "outputId": "ac02a1ff-acaf-4f85-cafe-ce40117f1e28"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "ModuleNotFoundError                       Traceback (most recent call last)",
      "Cell In[2], line 2\n",
      "      1 # Mount Google Drive\n",
      "----> 2 from google.colab import drive\n",
      "      3 drive.mount('/content/drive')\n",
      "      5 # Set up project directory\n",
      "ModuleNotFoundError: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up project directory\n",
    "PROJECT_DIR = '/content/drive/MyDrive/chart-hero'\n",
    "!mkdir -p {PROJECT_DIR}\n",
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0jRvg4sqK1g",
    "outputId": "444a5efc-589e-42a9-a676-e3e48c7f33d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/chart-hero/chart-hero\n",
      "remote: Enumerating objects: 7, done.\n",
      "remote: Counting objects: 100% (7/7), done.\n",
      "remote: Compressing objects: 100% (1/1), done.\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\n",
      "Unpacking objects: 100% (4/4), 350 bytes | 4.00 KiB/s, done.\n",
      "From https://github.com/maple-buice/chart-hero\n",
      "   a2a2fdf..6140821  main       -> origin/main\n",
      "Updating a2a2fdf..6140821\n",
      "Fast-forward\n",
      " model_training/train_transformer.py | 10 ----------\n",
      " 1 file changed, 10 deletions(-)\n",
      "/content/drive/MyDrive/chart-hero\n",
      "/content/drive/MyDrive/chart-hero\n"
     ]
    }
   ],
   "source": [
    "# Clone or update repository\n",
    "import os\n",
    "if not os.path.exists('chart-hero'):\n",
    "    !git clone https://github.com/maple-buice/chart-hero.git\n",
    "else:\n",
    "    %cd chart-hero\n",
    "    !git pull\n",
    "    %cd ..\n",
    "\n",
    "%cd chart-hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtD-YME7qK1h",
    "outputId": "05355eae-0df3-41a0-c459-7ebf78275796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 23.2/23.2 MB 44.3 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 875.6/875.6 kB 46.9 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.1/13.1 MB 57.6 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 663.9/663.9 MB 2.0 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 417.9/417.9 MB 3.0 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 168.4/168.4 MB 5.9 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 58.1/58.1 MB 14.1 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 128.2/128.2 MB 8.2 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 204.1/204.1 MB 3.9 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 147.8/147.8 MB 5.5 MB/s eta 0:00:00\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 99.1/99.1 kB 6.3 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ï¿½ï¿½â”â”â”â”â” 848.7/848.7 MB 1.5 MB/s eta 0:00:00\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 823.1/823.1 kB 12.7 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 962.5/962.5 kB 38.0 MB/s eta 0:00:00\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 58.0/58.0 kB 2.1 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 54.6/54.6 kB 3.1 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.8/4.8 MB 50.2 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 85.9/85.9 kB 6.2 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 260.1/260.1 kB 17.2 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 113.4/113.4 kB 7.1 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.6/1.6 MB 63.1 MB/s eta 0:00:00\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n",
      "WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/usr/local/lib/python3.11/dist-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "#!pip uninstall -y torch torchvision torchaudio fastai\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q pytorch-lightning transformers timm wandb\n",
    "!pip install -q librosa soundfile scikit-learn pandas numpy matplotlib seaborn tqdm\n",
    "!pip install -q ipywidgets mido pedalboard audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibgreACNqK1h"
   },
   "source": [
    "## 2. Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80Z9V6ccqK1h",
    "outputId": "080014e6-2835-48a9-c6a5-d0a45f01aadd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset setup...\n",
      "Google Drive already mounted at /content/drive.\n",
      "Project directory in Drive: /content/drive/MyDrive/chart-hero\n",
      "Dataset directory in Drive: /content/drive/MyDrive/chart-hero/datasets\n",
      "Dataset already successfully unzipped. Sentinel file found: /content/drive/MyDrive/chart-hero/datasets/.unzip_successful_sentinel\n",
      "Verified: Expected content 'e-gmd-v1.0.0' exists at '/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0'.\n",
      "--- Final Verification ---\n",
      "Checking existence of sentinel file: /content/drive/MyDrive/chart-hero/datasets/.unzip_successful_sentinel -> Exists\n",
      "Checking existence of ZIP file in Drive: /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0.zip -> Exists\n",
      "Checking existence of expected unzipped content: /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0 -> Exists\n",
      "Listing contents of '/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0' (first few items):\n",
      "- drummer5\n",
      "- drummer7\n",
      "- drummer6\n",
      "- e-gmd-v1.0.0.csv\n",
      "- LICENSE\n",
      "  ...\n",
      "Dataset setup cell execution complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from google.colab import drive\n",
    "\n",
    "# --- Configuration ---\n",
    "DRIVE_MOUNT_POINT = '/content/drive'\n",
    "MY_DRIVE_PATH = os.path.join(DRIVE_MOUNT_POINT, 'MyDrive')\n",
    "PROJECT_DIR_NAME = 'chart-hero' # As per your workspace structure\n",
    "PROJECT_DRIVE_PATH = os.path.join(MY_DRIVE_PATH, PROJECT_DIR_NAME)\n",
    "DATASET_DIR_NAME = 'datasets'\n",
    "DATASET_DIR = os.path.join(PROJECT_DRIVE_PATH, DATASET_DIR_NAME)\n",
    "\n",
    "# Dataset URL and paths\n",
    "# IMPORTANT: Please verify this URL and the expected unzipped content name for your specific dataset.\n",
    "DATASET_URL = \"https://storage.googleapis.com/magentadata/datasets/e-gmd/v1.0.0/e-gmd-v1.0.0.zip\"\n",
    "ZIP_FILE_NAME = os.path.basename(DATASET_URL)\n",
    "DRIVE_ZIP_PATH = os.path.join(DATASET_DIR, ZIP_FILE_NAME) # ZIP stored in Google Drive\n",
    "\n",
    "# Define the path for an expected file/folder after unzipping.\n",
    "# For e-gmd-v1.0.0.zip, it would be 'e-gmd-v1.0.0'.\n",
    "EXPECTED_UNZIPPED_CONTENT_NAME = \"e-gmd-v1.0.0\"\n",
    "EXPECTED_UNZIPPED_CONTENT_PATH = os.path.join(DATASET_DIR, EXPECTED_UNZIPPED_CONTENT_NAME)\n",
    "\n",
    "SENTINEL_FILE_NAME = \".unzip_successful_sentinel\"\n",
    "SENTINEL_FILE_PATH = os.path.join(DATASET_DIR, SENTINEL_FILE_NAME)\n",
    "\n",
    "# --- Setup ---\n",
    "print(\"Starting dataset setup...\")\n",
    "\n",
    "# 1. Mount Google Drive\n",
    "if not os.path.exists(MY_DRIVE_PATH) or not os.path.ismount(DRIVE_MOUNT_POINT):\n",
    "    print(f\"Mounting Google Drive at {DRIVE_MOUNT_POINT}...\")\n",
    "    drive.mount(DRIVE_MOUNT_POINT, force_remount=True) # force_remount can be useful\n",
    "else:\n",
    "    print(f\"Google Drive already mounted at {DRIVE_MOUNT_POINT}.\")\n",
    "\n",
    "# 2. Create project and dataset directories in Drive if they don't exist\n",
    "os.makedirs(PROJECT_DRIVE_PATH, exist_ok=True)\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "print(f\"Project directory in Drive: {PROJECT_DRIVE_PATH}\")\n",
    "print(f\"Dataset directory in Drive: {DATASET_DIR}\")\n",
    "\n",
    "\n",
    "# --- Main Logic ---\n",
    "\n",
    "# 3. Check if unzipping is already complete (sentinel file exists)\n",
    "if os.path.exists(SENTINEL_FILE_PATH):\n",
    "    print(f\"Dataset already successfully unzipped. Sentinel file found: {SENTINEL_FILE_PATH}\")\n",
    "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
    "        print(f\"Verified: Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
    "    else:\n",
    "        print(f\"WARNING: Sentinel file exists, but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
    "        print(\"This might indicate that the unzipped content was moved or deleted after the sentinel was created.\")\n",
    "        print(\"If you encounter issues, consider deleting the sentinel file and this cell's output, then re-running.\")\n",
    "else:\n",
    "    print(f\"Sentinel file not found at {SENTINEL_FILE_PATH}. Proceeding with dataset download and/or unzip.\")\n",
    "\n",
    "    # 4. Download the dataset ZIP if it doesn't exist in Google Drive\n",
    "    if not os.path.exists(DRIVE_ZIP_PATH):\n",
    "        print(f\"ZIP file not found at {DRIVE_ZIP_PATH}. Downloading...\")\n",
    "        # Download directly to Google Drive\n",
    "        download_command = f\"wget -O '{DRIVE_ZIP_PATH}' '{DATASET_URL}'\"\n",
    "        print(f\"Executing: {download_command}\")\n",
    "        process = subprocess.run(download_command, shell=True, capture_output=True, text=True)\n",
    "        if process.returncode == 0:\n",
    "            print(f\"Download successful. ZIP file saved to {DRIVE_ZIP_PATH}\")\n",
    "        else:\n",
    "            print(\"Download failed.\")\n",
    "            print(f\"Stdout: {process.stdout}\")\n",
    "            print(f\"Stderr: {process.stderr}\")\n",
    "            # Clean up potentially incomplete ZIP file\n",
    "            if os.path.exists(DRIVE_ZIP_PATH):\n",
    "                os.remove(DRIVE_ZIP_PATH)\n",
    "                print(f\"Removed potentially incomplete ZIP file: {DRIVE_ZIP_PATH}\")\n",
    "            raise Exception(f\"Failed to download dataset from {DATASET_URL} to {DRIVE_ZIP_PATH}\")\n",
    "    else:\n",
    "        print(f\"ZIP file already exists at {DRIVE_ZIP_PATH}. Skipping download.\")\n",
    "        file_size = os.path.getsize(DRIVE_ZIP_PATH)\n",
    "        print(f\"Existing ZIP file size: {file_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "\n",
    "    # 5. Unzip the dataset from Google Drive to Google Drive\n",
    "    print(f\"Checking for expected unzipped content at: {EXPECTED_UNZIPPED_CONTENT_PATH}\")\n",
    "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
    "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' already exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
    "        print(\"Unzip will attempt to complete by skipping existing files (due to -nq option).\")\n",
    "    else:\n",
    "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found. Proceeding with unzip.\")\n",
    "\n",
    "    print(f\"Unzipping '{DRIVE_ZIP_PATH}' to '{DATASET_DIR}'...\")\n",
    "    # -nq: never overwrite existing files (good for resuming, quiet)\n",
    "    unzip_command = f\"unzip -nq '{DRIVE_ZIP_PATH}' -d '{DATASET_DIR}'\"\n",
    "    print(f\"Executing: {unzip_command}\")\n",
    "    process = subprocess.run(unzip_command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    # Check unzip outcome\n",
    "    # unzip return codes:\n",
    "    # 0: success\n",
    "    # 1: warning error (e.g., some files not processed, or -nq found existing files)\n",
    "    # Other codes: more serious errors\n",
    "    if process.returncode == 0 or process.returncode == 1:\n",
    "        print(\"Unzip command executed.\")\n",
    "        if process.stdout: print(f\"Unzip stdout: {process.stdout[:500]}...\") # Print some output\n",
    "        if process.stderr: print(f\"Unzip stderr: {process.stderr[:500]}...\")\n",
    "\n",
    "        # Verify expected content after unzip attempt\n",
    "        if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
    "            print(f\"Unzip appears successful. Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
    "            # Create sentinel file\n",
    "            print(f\"Creating sentinel file: {SENTINEL_FILE_PATH}\")\n",
    "            with open(SENTINEL_FILE_PATH, 'w') as f:\n",
    "                f.write(f\"Unzip completed successfully for {ZIP_FILE_NAME} into {DATASET_DIR} on {subprocess.check_output(['date']).decode('utf-8').strip()}\n")\n",
    "            print(\"Sentinel file created.\")\n",
    "        else:\n",
    "            print(f\"ERROR: Unzip command finished (exit code {process.returncode}), but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' still NOT found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
    "            print(\"This could indicate an issue with the ZIP file structure, an empty ZIP, or the unzip process itself did not extract the main folder as expected.\")\n",
    "            print(f\"Please check the ZIP file contents and the '{EXPECTED_UNZIPPED_CONTENT_NAME}' variable if this is incorrect.\")\n",
    "            # Do NOT create sentinel file if primary content is missing.\n",
    "    else:\n",
    "        print(f\"Unzip failed with exit code {process.returncode}.\")\n",
    "        print(f\"Stdout: {process.stdout}\")\n",
    "        print(f\"Stderr: {process.stderr}\")\n",
    "        raise Exception(f\"Failed to unzip dataset from {DRIVE_ZIP_PATH}. Check logs for details.\")\n",
    "\n",
    "# --- Verification (Final Check) ---\n",
    "print(\"--- Final Verification ---\")\n",
    "print(f\"Checking existence of sentinel file: {SENTINEL_FILE_PATH} -> {'Exists' if os.path.exists(SENTINEL_FILE_PATH) else 'Not found'}\")\n",
    "print(f\"Checking existence of ZIP file in Drive: {DRIVE_ZIP_PATH} -> {'Exists' if os.path.exists(DRIVE_ZIP_PATH) else 'Not found'}\")\n",
    "print(f\"Checking existence of expected unzipped content: {EXPECTED_UNZIPPED_CONTENT_PATH} -> {'Exists' if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH) else 'Not found'}\")\n",
    "\n",
    "if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
    "    print(f\"Listing contents of '{EXPECTED_UNZIPPED_CONTENT_PATH}' (first few items):\")\n",
    "    try:\n",
    "        contents = os.listdir(EXPECTED_UNZIPPED_CONTENT_PATH)\n",
    "        for item in contents[:5]:\n",
    "            print(f\"- {item}\")\n",
    "        if len(contents) > 5:\n",
    "            print(\"  ...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not list contents: {e}\")\n",
    "else:\n",
    "    print(f\"Cannot list contents as '{EXPECTED_UNZIPPED_CONTENT_PATH}' does not exist.\")\n",
    "\n",
    "print(\"Dataset setup cell execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343,
     "referenced_widgets": [
      "5a190b07e2bc4b82938c7a5d5aa2500c",
      "8b95cac3e8f9499696431954b6c741c9",
      "963debc898394f85a0665c99816b5c93",
      "a68e9bf392054f7ebac43e2bdb275353",
      "e7791ef27dab4b719035fc0c4b48c1bd",
      "23e3bfd6824148cc8c275ac5ce1968f5",
      "62936ff3a9674fb59b0ec7e2c27c0196",
      "e90a551ef9d2448c8d9fb786440205f1",
      "bb8bc263a0124757954b43941e76fea8",
      "665cb563cab94b738b6febadb598279c",
      "804ef658e5ae43659e155e92748617b5"
     ]
    },
    "id": "hBCDd7MDqK1h",
    "outputId": "cbb2b644-d568-41b5-9e32-73acdc13dad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing raw EGMD data...\n",
      "Filtering out the midi/audio pair that has a duration difference > 1.0 second using soundfile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a190b07e2bc4b82938c7a5d5aa2500c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating audio durations sequentially:   0%|          | 0/45537 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "KeyboardInterrupt                         Traceback (most recent call last)",
      "<ipython-input-10-1033492793> in <cell line: 0>()\n",
      "     16 \n",
      "     17     # Create data preparation instance\n",
      "---> 18     data_prep = data_preparation(\n",
      "     19         directory_path=egmd_dir,\n",
      "     20         dataset='egmd',\n",
      "/content/drive/MyDrive/chart-hero/src/chart_hero/model_training/data_preparation.py in __init__(self, directory_path, dataset, sample_ratio, diff_threshold, n_jobs)\n",
      "     84             durations = []\n",
      "     85             for i, filename in enumerate(tqdm(df['audio_filename'], desc=\"Calculating audio durations sequentially\")):\n",
      "---> 86                 duration = get_wav_duration(filename, self.directory_path)\n",
      "     87                 durations.append(duration)\n",
      "     88 \n",
      "/content/drive/MyDrive/chart-hero/src/chart_hero/model_training/data_preparation.py in get_wav_duration(filepath, base_dir)\n",
      "     74             def get_wav_duration(filepath, base_dir): # Pass base_dir explicitly\n",
      "     75                 try:\n",
      "---> 76                     info = sf.info(os.path.join(base_dir, filepath))\n",
      "     77                     return info.duration\n",
      "     78                 except Exception as e:\n",
      "/usr/local/lib/python3.11/dist-packages/soundfile.py in info(file, verbose)\n",
      "    486         Whether to print additional information.\n",
      "    487     \"\"\"\n",
      "---> 488     return _SoundFileInfo(file, verbose)\n",
      "    489 \n",
      "    490 \n",
      "/usr/local/lib/python3.11/dist-packages/soundfile.py in __init__(self, file, verbose)\n",
      "    431     def __init__(self, file, verbose):\n",
      "    432         self.verbose = verbose\n",
      "---> 433         with SoundFile(file) as f:\n",
      "    434             self.name = f.name\n",
      "    435             self.samplerate = f.samplerate\n",
      "/usr/local/lib/python3.11/dist-packages/soundfile.py in __init__(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\n",
      "    688         self._info = _create_info_struct(file, mode, samplerate, channels,\n",
      "    689                                          format, subtype, endian)\n",
      "---> 690         self._file = self._open(file, mode_int, closefd)\n",
      "    691         if set(mode).issuperset('r+'):\n",
      "    692             # Move write position to 0 (like in Python file objects)\n",
      "/usr/local/lib/python3.11/dist-packages/soundfile.py in _open(self, file, mode_int, closefd)\n",
      "   1240         \"\"\"Call the appropriate sf_open*() function from libsndfile.\"\"\"\n",
      "   1241         if isinstance(file, (_unicode, bytes)):\n",
      "-> 1242             if _os.path.isfile(file):\n",
      "   1243                 if 'x' in self.mode:\n",
      "   1244                     raise OSError(\"File exists: {0!r}\".format(self.name))\n",
      "/usr/lib/python3.11/genericpath.py in isfile(path)\n",
      "KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "# Prepare training data (if not already processed)\n",
    "# This cell converts the raw EGMD data to transformer-compatible format\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/chart-hero/src')\n",
    "\n",
    "from chart_hero.model_training.data_preparation import data_preparation\n",
    "from chart_hero.model_training.transformer_data import convert_legacy_data\n",
    "\n",
    "# Process raw EGMD data\n",
    "egmd_dir = \"/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0\"\n",
    "processed_dir = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
    "\n",
    "if not os.path.exists(processed_dir):\n",
    "    print(\"Processing raw EGMD data...\")\n",
    "\n",
    "    # Create data preparation instance\n",
    "    data_prep = data_preparation(\n",
    "        directory_path=egmd_dir,\n",
    "        dataset='egmd',\n",
    "        sample_ratio=1.0,\n",
    "        diff_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # Create audio set with batching\n",
    "    data_prep.create_audio_set(\n",
    "        pad_before=0.1,\n",
    "        pad_after=0.1,\n",
    "        fix_length=10.0,  # 10 second segments\n",
    "        batching=True,\n",
    "        dir_path=processed_dir,\n",
    "        num_batches=20,\n",
    "        memory_limit_gb=50  # Optimized for High-RAM: Enable parallel processing\n",
    "    )\n",
    "\n",
    "    print(\"Data processing completed!\")\n",
    "else:\n",
    "    print(\"Processed data already exists.\")\n",
    "\n",
    "!ls -la {processed_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4GOGPreqK1h"
   },
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSmGXtnAqK1h",
    "outputId": "22257298-4806-413d-e507-ccfcac52f488"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up W&B logging\n",
    "import wandb\n",
    "\n",
    "# Login to W&B (you'll need to provide your API key)\n",
    "wandb.login()\n",
    "\n",
    "# Or set the API key directly\n",
    "# wandb.login(key=\"your-wandb-api-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fe08HFvqK1h",
    "outputId": "1ba80e84-e945-4f97-9807-3fc215b21716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:==================================================\n",
      "INFO:__main__:TRANSFORMER SETUP TESTS\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:\n",
      "------------------------------\n",
      "INFO:__main__:Running Configuration test (using 'cloud' config context)..\n",
      "INFO:__main__:------------------------------\n",
      "INFO:__main__:Testing configuration classes...\n",
      "INFO:__main__:--- Validating invoked config: 'cloud' ---\n",
      "INFO:__main__:âœ“ Invoked config 'cloud' (resolved to cloud): VALIDATED - device cuda, batch_size=64\n",
      "INFO:__main__:--- Checking standard config: 'local' ---\n",
      "WARNING:__main__:âœ“ Config 'local' (device: mps) specifies MPS, but MPS not available. Loaded but skipping full device validation for this non-invoked config.\n",
      "INFO:__main__:--- Checking standard config: 'auto' ---\n",
      "INFO:__main__:Auto-detected config for verification: CloudConfig\n",
      "INFO:__main__:âœ“ Config 'auto': Loaded & Validated - device cuda, batch_size=64\n",
      "INFO:__main__:âœ“ Configuration test PASSED\n",
      "INFO:__main__:\n",
      "------------------------------\n",
      "INFO:__main__:Running Model Architecture test (using 'cloud' config context)..\n",
      "INFO:__main__:------------------------------\n",
      "INFO:__main__:Testing model architecture...\n",
      "Model created with 7,214,217 total parameters\n",
      "Trainable parameters: 7,214,217\n",
      "INFO:__main__:âœ“ Input shape: torch.Size([2, 1, 256, 128])\n",
      "INFO:__main__:âœ“ Output logits shape: torch.Size([2, 9])\n",
      "INFO:__main__:âœ“ CLS embedding shape: torch.Size([2, 384])\n",
      "INFO:__main__:âœ“ Model Architecture test PASSED\n",
      "INFO:__main__:\n",
      "------------------------------\n",
      "INFO:__main__:Running Data Processing test (using 'cloud' config context)..\n",
      "INFO:__main__:------------------------------\n",
      "INFO:__main__:Testing data processing...\n",
      "INFO:__main__:âœ“ Audio shape: torch.Size([1, 264600])\n",
      "INFO:__main__:âœ“ Spectrogram shape: torch.Size([1, 517, 128])\n",
      "INFO:__main__:âœ“ Padded spectrogram shape: torch.Size([1, 528, 128])\n",
      "INFO:__main__:âœ“ Patch shape: (33, 8)\n",
      "INFO:model_training.transformer_data:Created test dataset with 5 samples\n",
      "INFO:__main__:âœ“ Dataset created with 5 samples\n",
      "WARNING:model_training.transformer_data:Error processing sample 0: 'audio_wav'\n",
      "INFO:__main__:âœ“ Sample spectrogram shape: torch.Size([1, 528, 128])\n",
      "INFO:__main__:âœ“ Sample labels shape: torch.Size([9])\n",
      "INFO:__main__:âœ“ Data Processing test PASSED\n",
      "INFO:__main__:\n",
      "------------------------------\n",
      "INFO:__main__:Running Training Module test (using 'cloud' config context)..\n",
      "INFO:__main__:------------------------------\n",
      "INFO:__main__:Testing training module...\n",
      "Model created with 7,214,217 total parameters\n",
      "Trainable parameters: 7,214,217\n",
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py:447: You are trying to `self.log()` but `Trainer(barebones=True)` is configured. Logging can impact raw speed so it is disabled under this setting.\n",
      "INFO:__main__:âœ“ Training step loss: 1.0013\n",
      "INFO:__main__:âœ“ Validation step loss: 1.0117\n",
      "INFO:__main__:âœ“ Optimizer configured: AdamW, Scheduler: SequentialLR\n",
      "INFO:__main__:âœ“ Training Module test PASSED\n",
      "INFO:__main__:\n",
      "==================================================\n",
      "INFO:__main__:TEST SUMMARY\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:Configuration: PASS\n",
      "INFO:__main__:Model Architecture: PASS\n",
      "INFO:__main__:Data Processing: PASS\n",
      "INFO:__main__:Training Module: PASS\n",
      "INFO:__main__:\n",
      "Overall: 4/4 tests passed\n",
      "INFO:__main__:ðŸŽ‰ All tests passed! Transformer setup is ready.\n"
     ]
    }
   ],
   "source": [
    "# Test transformer setup\n",
    "!python src/chart_hero/model_training/test_transformer_setup.py --config cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SFZlBqCqK1h",
    "outputId": "bc36e65e-d1dd-4c7b-e51f-8ba8baeda163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:No --experiment-tag provided by CLI. Using generated tag for this run: 20250611_220546\n",
      "INFO:__main__:Loading configuration profile: cloud\n",
      "INFO:__main__:Overriding data_dir with CLI argument: /content/drive/MyDrive/chart-hero/datasets/processed\n",
      "INFO:__main__:Overriding audio_dir with CLI argument: /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0\n",
      "INFO:__main__:Model checkpoints will be saved in: /content/drive/MyDrive/chart-hero/chart-hero/chart-hero/chart-hero/chart-hero/chart-hero/chart-hero/model_training/transformer_models/20250611_220546\n",
      "INFO:__main__:Experiment-specific logging configured at: /content/drive/MyDrive/chart-hero/chart-hero/chart-hero/chart-hero/chart-hero/chart-hero/chart-hero/logs/training_20250611_220546.log\n",
      "ERROR:__main__:Data directory does not exist: /content/drive/MyDrive/chart-hero/datasets/processed\n"
     ]
    }
   ],
   "source": [
    "# Start training with cloud configuration\n",
    "DATA_DIR = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
    "AUDIO_DIR = \"/content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0\"\n",
    "\n",
    "!python src/chart_hero/model_training/train_transformer.py \\\n",
    "    --config cloud \\\n",
    "    --data-dir {DATA_DIR} \\\n",
    "    --audio-dir {AUDIO_DIR} \\\n",
    "    --project-name chart-hero-transformer-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bDkyd8nqK1h"
   },
   "source": [
    "## 4. Resume Training (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TD03wwEqK1h"
   },
   "outputs": [],
   "source": [
    "# Resume from checkpoint\n",
    "CHECKPOINT_PATH = \"/content/drive/MyDrive/chart-hero/models/last.ckpt\"\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    !python src/chart_hero/model_training/train_transformer.py \\\n",
    "        --config cloud \\\n",
    "        --data-dir {DATA_DIR} \\\n",
    "        --audio-dir {AUDIO_DIR} \\\n",
    "        --resume {CHECKPOINT_PATH} \\\n",
    "        --project-name chart-hero-transformer-colab\n",
    "else:\n",
    "    print(f\"Checkpoint not found: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_jtaL-mqK1h"
   },
   "source": [
    "## 5. Model Evaluation and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaRwU8F9qK1h",
    "outputId": "e2aa6e56-7da3-4d51-907e-db54a8fb21a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model not found: /content/drive/MyDrive/chart-hero/models/best_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate best model\n",
    "import torch\n",
    "from chart_hero.model_training.train_transformer import DrumTranscriptionModule\n",
    "from chart_hero.model_training.transformer_config import get_config\n",
    "\n",
    "config = get_config(\"cloud\")\n",
    "best_model_path = \"/content/drive/MyDrive/chart-hero/models/best_model.ckpt\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    model = DrumTranscriptionModule.load_from_checkpoint(best_model_path)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Export to ONNX for deployment\n",
    "    dummy_input = torch.randn(1, 1, 256, 128)\n",
    "    onnx_path = \"/content/drive/MyDrive/chart-hero/models/drum_transformer.onnx\"\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model.model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['spectrogram'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={\n",
    "            'spectrogram': {0: 'batch_size', 2: 'time'},\n",
    "            'logits': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Model exported to ONNX: {onnx_path}\")\n",
    "else:\n",
    "    print(f\"Best model not found: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9bLGHPNqK1h"
   },
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSpRRAa1qK1h"
   },
   "outputs": [],
   "source": [
    "# Clean up temporary files and finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "# Show final model and log locations\n",
    "print(\"Training completed!\")\n",
    "print(f\"Models saved to: /content/drive/MyDrive/chart-hero/models/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/chart-hero/logs/\")\n",
    "print(f\"Datasets saved to: /content/drive/MyDrive/chart-hero/datasets/\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 }
}
