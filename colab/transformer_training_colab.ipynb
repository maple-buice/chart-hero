{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maple-buice/chart-hero/blob/main/colab/transformer_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUQ1odMpqK1f"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJsmeNtbqK1g",
        "outputId": "8c2a628b-d2df-49b1-ab3b-9fdcdc9ca52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May 27 12:13:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwo6JqcWqK1g",
        "outputId": "4b420a13-3e3d-446f-c580-9715f3680c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/chart-hero\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up project directory\n",
        "PROJECT_DIR = '/content/drive/MyDrive/chart-hero'\n",
        "!mkdir -p {PROJECT_DIR}\n",
        "%cd {PROJECT_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0jRvg4sqK1g",
        "outputId": "f20954a9-eae1-43d7-9313-5f69eca46554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/chart-hero/chart-hero\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 4), reused 4 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (8/8), 2.01 KiB | 3.00 KiB/s, done.\n",
            "From https://github.com/maple-buice/chart-hero\n",
            "   b8042ad..c5df979  main       -> origin/main\n",
            "Updating b8042ad..c5df979\n",
            "Fast-forward\n",
            " colab/transformer_training_colab.ipynb | 92 \u001b[32m++++++++++++++++++++++++++\u001b[m\u001b[31m--------\u001b[m\n",
            " 1 file changed, 71 insertions(+), 21 deletions(-)\n",
            "/content/drive/MyDrive/chart-hero\n",
            "/content/drive/MyDrive/chart-hero/chart-hero\n"
          ]
        }
      ],
      "source": [
        "# Clone or update repository\n",
        "import os\n",
        "if not os.path.exists('chart-hero'):\n",
        "    !git clone https://github.com/maple-buice/chart-hero.git\n",
        "else:\n",
        "    %cd chart-hero\n",
        "    !git pull\n",
        "    %cd ..\n",
        "\n",
        "%cd chart-hero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtD-YME7qK1h",
        "outputId": "7cbe9c04-0db0-4394-c5dc-9ee6f2278760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q pytorch-lightning transformers timm wandb\n",
        "!pip install -q librosa soundfile scikit-learn pandas numpy matplotlib seaborn tqdm\n",
        "!pip install -q ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibgreACNqK1h"
      },
      "source": [
        "## 2. Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Z9V6ccqK1h",
        "outputId": "c984337f-9e4a-46da-e89b-1cb3fd20c02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. Download Step ---\n",
            "Downloading https://storage.googleapis.com/magentadata/datasets/e-gmd/v1.0.0/e-gmd-v1.0.0.zip to instance storage (/content/e-gmd-v1.0.0.zip)...\n",
            "--2025-05-27 16:22:04--  https://storage.googleapis.com/magentadata/datasets/e-gmd/v1.0.0/e-gmd-v1.0.0.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96422999145 (90G) [application/zip]\n",
            "Saving to: ‘/content/e-gmd-v1.0.0.zip’\n",
            "\n",
            "zip                   0%[                    ] 343.84M  18.4MB/s    eta 84m 31s^C\n",
            "Download complete.\n",
            "\n",
            "--- 2. Unzip Step ---\n",
            "Unzipping /content/e-gmd-v1.0.0.zip to /content/drive/MyDrive/chart-hero/datasets...\n",
            "[/content/e-gmd-v1.0.0.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/e-gmd-v1.0.0.zip or\n",
            "        /content/e-gmd-v1.0.0.zip.zip, and cannot find /content/e-gmd-v1.0.0.zip.ZIP, period.\n",
            "Unzipping may have failed. Expected content not found at /content/drive/MyDrive/chart-hero/datasets/e-gmd-v1.0.0.\n",
            "Temporary ZIP file /content/e-gmd-v1.0.0.zip will be kept for inspection.\n",
            "\n",
            "Final check - Dataset directory contents (/content/drive/MyDrive/chart-hero/datasets):\n",
            "total 94163086\n",
            "-rw------- 1 root root 96422999145 Mar 10  2020 e-gmd-v1.0.0.zip\n",
            "\n",
            "Final check - Instance /content directory contents:\n",
            "total 352520\n",
            "drwxr-xr-x 1 root root      4096 May 27 16:22 .\n",
            "drwxr-xr-x 1 root root      4096 May 27 12:38 ..\n",
            "drwxr-xr-x 4 root root      4096 May 14 13:38 .config\n",
            "drwx------ 7 root root      4096 May 27 12:46 drive\n",
            "-rw-r--r-- 1 root root 360955904 May 27 16:22 e-gmd-v1.0.0.zip\n",
            "drwxr-xr-x 1 root root      4096 May 14 13:38 sample_data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_MOUNT_POINT = '/content/drive'\n",
        "MY_DRIVE_PATH = os.path.join(DRIVE_MOUNT_POINT, 'MyDrive')\n",
        "PROJECT_DIR_NAME = 'chart-hero' # As per your workspace structure\n",
        "PROJECT_DRIVE_PATH = os.path.join(MY_DRIVE_PATH, PROJECT_DIR_NAME)\n",
        "DATASET_DIR_NAME = 'datasets'\n",
        "DATASET_DIR = os.path.join(PROJECT_DRIVE_PATH, DATASET_DIR_NAME)\n",
        "\n",
        "# Dataset URL and paths\n",
        "# IMPORTANT: Please verify this URL and the expected unzipped content name for your specific dataset.\n",
        "# This example uses the Groove MIDI dataset.\n",
        "DATASET_URL = \"https://storage.googleapis.com/magentadata/datasets/groove/groove-midi-dataset-v1.0.0.zip\"\n",
        "ZIP_FILE_NAME = os.path.basename(DATASET_URL)\n",
        "DRIVE_ZIP_PATH = os.path.join(DATASET_DIR, ZIP_FILE_NAME) # ZIP stored in Google Drive\n",
        "\n",
        "# Define the path for an expected file/folder after unzipping.\n",
        "# For groove-midi-dataset-v1.0.0.zip, the main folder is 'groove'.\n",
        "# For e-gmd-v1.0.0.zip, it would be 'e-gmd-v1.0.0'.\n",
        "EXPECTED_UNZIPPED_CONTENT_NAME = \"groove\" # ADJUST THIS if using a different dataset\n",
        "EXPECTED_UNZIPPED_CONTENT_PATH = os.path.join(DATASET_DIR, EXPECTED_UNZIPPED_CONTENT_NAME)\n",
        "\n",
        "SENTINEL_FILE_NAME = \".unzip_successful_sentinel\"\n",
        "SENTINEL_FILE_PATH = os.path.join(DATASET_DIR, SENTINEL_FILE_NAME)\n",
        "\n",
        "# --- Setup ---\n",
        "print(\"Starting dataset setup...\")\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "if not os.path.exists(MY_DRIVE_PATH) or not os.path.ismount(DRIVE_MOUNT_POINT):\n",
        "    print(f\"Mounting Google Drive at {DRIVE_MOUNT_POINT}...\")\n",
        "    drive.mount(DRIVE_MOUNT_POINT, force_remount=True) # force_remount can be useful\n",
        "else:\n",
        "    print(f\"Google Drive already mounted at {DRIVE_MOUNT_POINT}.\")\n",
        "\n",
        "# 2. Create project and dataset directories in Drive if they don't exist\n",
        "os.makedirs(PROJECT_DRIVE_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "print(f\"Project directory in Drive: {PROJECT_DRIVE_PATH}\")\n",
        "print(f\"Dataset directory in Drive: {DATASET_DIR}\")\n",
        "\n",
        "\n",
        "# --- Main Logic ---\n",
        "\n",
        "# 3. Check if unzipping is already complete (sentinel file exists)\n",
        "if os.path.exists(SENTINEL_FILE_PATH):\n",
        "    print(f\"Dataset already successfully unzipped. Sentinel file found: {SENTINEL_FILE_PATH}\")\n",
        "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "        print(f\"Verified: Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Sentinel file exists, but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "        print(\"This might indicate that the unzipped content was moved or deleted after the sentinel was created.\")\n",
        "        print(\"If you encounter issues, consider deleting the sentinel file and this cell's output, then re-running.\")\n",
        "else:\n",
        "    print(f\"Sentinel file not found at {SENTINEL_FILE_PATH}. Proceeding with dataset download and/or unzip.\")\n",
        "\n",
        "    # 4. Download the dataset ZIP if it doesn't exist in Google Drive\n",
        "    if not os.path.exists(DRIVE_ZIP_PATH):\n",
        "        print(f\"ZIP file not found at {DRIVE_ZIP_PATH}. Downloading...\")\n",
        "        # Download directly to Google Drive\n",
        "        download_command = f\"wget -O '{DRIVE_ZIP_PATH}' '{DATASET_URL}'\"\n",
        "        print(f\"Executing: {download_command}\")\n",
        "        process = subprocess.run(download_command, shell=True, capture_output=True, text=True)\n",
        "        if process.returncode == 0:\n",
        "            print(f\"Download successful. ZIP file saved to {DRIVE_ZIP_PATH}\")\n",
        "        else:\n",
        "            print(\"Download failed.\")\n",
        "            print(f\"Stdout: {process.stdout}\")\n",
        "            print(f\"Stderr: {process.stderr}\")\n",
        "            # Clean up potentially incomplete ZIP file\n",
        "            if os.path.exists(DRIVE_ZIP_PATH):\n",
        "                os.remove(DRIVE_ZIP_PATH)\n",
        "                print(f\"Removed potentially incomplete ZIP file: {DRIVE_ZIP_PATH}\")\n",
        "            raise Exception(f\"Failed to download dataset from {DATASET_URL} to {DRIVE_ZIP_PATH}\")\n",
        "    else:\n",
        "        print(f\"ZIP file already exists at {DRIVE_ZIP_PATH}. Skipping download.\")\n",
        "        file_size = os.path.getsize(DRIVE_ZIP_PATH)\n",
        "        print(f\"Existing ZIP file size: {file_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "\n",
        "    # 5. Unzip the dataset from Google Drive to Google Drive\n",
        "    print(f\"Checking for expected unzipped content at: {EXPECTED_UNZIPPED_CONTENT_PATH}\")\n",
        "    if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' already exists at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "        print(\"Unzip will attempt to complete by skipping existing files (due to -nq option).\")\n",
        "    else:\n",
        "        print(f\"Main expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' not found. Proceeding with unzip.\")\n",
        "\n",
        "    print(f\"Unzipping '{DRIVE_ZIP_PATH}' to '{DATASET_DIR}'...\")\n",
        "    # -nq: never overwrite existing files (good for resuming, quiet)\n",
        "    unzip_command = f\"unzip -nq '{DRIVE_ZIP_PATH}' -d '{DATASET_DIR}'\"\n",
        "    print(f\"Executing: {unzip_command}\")\n",
        "    process = subprocess.run(unzip_command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    # Check unzip outcome\n",
        "    # unzip return codes:\n",
        "    # 0: success\n",
        "    # 1: warning error (e.g., some files not processed, or -nq found existing files)\n",
        "    # Other codes: more serious errors\n",
        "    if process.returncode == 0 or process.returncode == 1:\n",
        "        print(\"Unzip command executed.\")\n",
        "        if process.stdout: print(f\"Unzip stdout: {process.stdout[:500]}...\") # Print some output\n",
        "        if process.stderr: print(f\"Unzip stderr: {process.stderr[:500]}...\")\n",
        "\n",
        "        # Verify expected content after unzip attempt\n",
        "        if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "            print(f\"Unzip appears successful. Expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "            # Create sentinel file\n",
        "            print(f\"Creating sentinel file: {SENTINEL_FILE_PATH}\")\n",
        "            with open(SENTINEL_FILE_PATH, 'w') as f:\n",
        "                f.write(f\"Unzip completed successfully for {ZIP_FILE_NAME} into {DATASET_DIR} on {subprocess.check_output(['date']).decode('utf-8').strip()}\")\n",
        "            print(\"Sentinel file created.\")\n",
        "        else:\n",
        "            print(f\"ERROR: Unzip command finished (exit code {process.returncode}), but expected content '{EXPECTED_UNZIPPED_CONTENT_NAME}' still NOT found at '{EXPECTED_UNZIPPED_CONTENT_PATH}'.\")\n",
        "            print(\"This could indicate an issue with the ZIP file structure, an empty ZIP, or the unzip process itself did not extract the main folder as expected.\")\n",
        "            print(f\"Please check the ZIP file contents and the '{EXPECTED_UNZIPPED_CONTENT_NAME}' variable if this is incorrect.\")\n",
        "            # Do NOT create sentinel file if primary content is missing.\n",
        "    else:\n",
        "        print(f\"Unzip failed with exit code {process.returncode}.\")\n",
        "        print(f\"Stdout: {process.stdout}\")\n",
        "        print(f\"Stderr: {process.stderr}\")\n",
        "        raise Exception(f\"Failed to unzip dataset from {DRIVE_ZIP_PATH}. Check logs for details.\")\n",
        "\n",
        "# --- Verification (Final Check) ---\n",
        "print(\"--- Final Verification ---\")\n",
        "print(f\"Checking existence of sentinel file: {SENTINEL_FILE_PATH} -> {'Exists' if os.path.exists(SENTINEL_FILE_PATH) else 'Not found'}\")\n",
        "print(f\"Checking existence of ZIP file in Drive: {DRIVE_ZIP_PATH} -> {'Exists' if os.path.exists(DRIVE_ZIP_PATH) else 'Not found'}\")\n",
        "print(f\"Checking existence of expected unzipped content: {EXPECTED_UNZIPPED_CONTENT_PATH} -> {'Exists' if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH) else 'Not found'}\")\n",
        "\n",
        "if os.path.exists(EXPECTED_UNZIPPED_CONTENT_PATH):\n",
        "    print(f\"Listing contents of '{EXPECTED_UNZIPPED_CONTENT_PATH}' (first few items):\")\n",
        "    try:\n",
        "        contents = os.listdir(EXPECTED_UNZIPPED_CONTENT_PATH)\n",
        "        for item in contents[:5]:\n",
        "            print(f\"- {item}\")\n",
        "        if len(contents) > 5:\n",
        "            print(\"  ...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not list contents: {e}\")\n",
        "else:\n",
        "    print(f\"Cannot list contents as '{EXPECTED_UNZIPPED_CONTENT_PATH}' does not exist.\")\n",
        "\n",
        "print(\"Dataset setup cell execution complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBCDd7MDqK1h"
      },
      "outputs": [],
      "source": [
        "# Prepare training data (if not already processed)\n",
        "# This cell converts the raw EGMD data to transformer-compatible format\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/chart-hero')\n",
        "\n",
        "from model_training.data_preparation import data_preparation\n",
        "from model_training.transformer_data import convert_legacy_data\n",
        "\n",
        "# Process raw EGMD data\n",
        "egmd_dir = \"/content/drive/MyDrive/chart-hero/datasets/expanded-groove-midi\"\n",
        "processed_dir = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
        "\n",
        "if not os.path.exists(processed_dir):\n",
        "    print(\"Processing raw EGMD data...\")\n",
        "\n",
        "    # Create data preparation instance\n",
        "    data_prep = data_preparation(\n",
        "        directory_path=egmd_dir,\n",
        "        dataset='egmd',\n",
        "        sample_ratio=1.0,\n",
        "        diff_threshold=1.0\n",
        "    )\n",
        "\n",
        "    # Create audio set with batching\n",
        "    data_prep.create_audio_set(\n",
        "        pad_before=0.1,\n",
        "        pad_after=0.1,\n",
        "        fix_length=10.0,  # 10 second segments\n",
        "        batching=True,\n",
        "        dir_path=processed_dir,\n",
        "        num_batches=20\n",
        "    )\n",
        "\n",
        "    print(\"Data processing completed!\")\n",
        "else:\n",
        "    print(\"Processed data already exists.\")\n",
        "\n",
        "!ls -la {processed_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4GOGPreqK1h"
      },
      "source": [
        "## 3. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSmGXtnAqK1h"
      },
      "outputs": [],
      "source": [
        "# Set up W&B logging\n",
        "import wandb\n",
        "\n",
        "# Login to W&B (you'll need to provide your API key)\n",
        "wandb.login()\n",
        "\n",
        "# Or set the API key directly\n",
        "# wandb.login(key=\"your-wandb-api-key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fe08HFvqK1h"
      },
      "outputs": [],
      "source": [
        "# Test transformer setup\n",
        "!python model_training/test_transformer_setup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SFZlBqCqK1h"
      },
      "outputs": [],
      "source": [
        "# Start training with cloud configuration\n",
        "DATA_DIR = \"/content/drive/MyDrive/chart-hero/datasets/processed\"\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/chart-hero/datasets/expanded-groove-midi\"\n",
        "\n",
        "!python model_training/train_transformer.py \\\n",
        "    --config cloud \\\n",
        "    --data-dir {DATA_DIR} \\\n",
        "    --audio-dir {AUDIO_DIR} \\\n",
        "    --project-name chart-hero-transformer-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bDkyd8nqK1h"
      },
      "source": [
        "## 4. Resume Training (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TD03wwEqK1h"
      },
      "outputs": [],
      "source": [
        "# Resume from checkpoint\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/chart-hero/models/last.ckpt\"\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    !python model_training/train_transformer.py \\\n",
        "        --config cloud \\\n",
        "        --data-dir {DATA_DIR} \\\n",
        "        --audio-dir {AUDIO_DIR} \\\n",
        "        --resume {CHECKPOINT_PATH} \\\n",
        "        --project-name chart-hero-transformer-colab\n",
        "else:\n",
        "    print(f\"Checkpoint not found: {CHECKPOINT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_jtaL-mqK1h"
      },
      "source": [
        "## 5. Model Evaluation and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaRwU8F9qK1h"
      },
      "outputs": [],
      "source": [
        "# Load and evaluate best model\n",
        "import torch\n",
        "from model_training.train_transformer import DrumTranscriptionModule\n",
        "from model_training.transformer_config import get_config\n",
        "\n",
        "config = get_config(\"cloud\")\n",
        "best_model_path = \"/content/drive/MyDrive/chart-hero/models/best_model.ckpt\"\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    model = DrumTranscriptionModule.load_from_checkpoint(best_model_path)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Export to ONNX for deployment\n",
        "    dummy_input = torch.randn(1, 1, 256, 128)\n",
        "    onnx_path = \"/content/drive/MyDrive/chart-hero/models/drum_transformer.onnx\"\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model.model,\n",
        "        dummy_input,\n",
        "        onnx_path,\n",
        "        export_params=True,\n",
        "        opset_version=11,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['spectrogram'],\n",
        "        output_names=['logits'],\n",
        "        dynamic_axes={\n",
        "            'spectrogram': {0: 'batch_size', 2: 'time'},\n",
        "            'logits': {0: 'batch_size'}\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"Model exported to ONNX: {onnx_path}\")\n",
        "else:\n",
        "    print(f\"Best model not found: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9bLGHPNqK1h"
      },
      "source": [
        "## 6. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSpRRAa1qK1h"
      },
      "outputs": [],
      "source": [
        "# Clean up temporary files and finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "# Show final model and log locations\n",
        "print(\"Training completed!\")\n",
        "print(f\"Models saved to: /content/drive/MyDrive/chart-hero/models/\")\n",
        "print(f\"Logs saved to: /content/drive/MyDrive/chart-hero/logs/\")\n",
        "print(f\"Datasets saved to: /content/drive/MyDrive/chart-hero/datasets/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
